akka.jvm-exit-on-fatal-error=off
akka {
  actor {
    provider = "cluster"
  }
  remote {
    log-remote-lifecycle-events = off
    netty.tcp {
      hostname = ${clusterAddr}
      port = ${clusterPort}
    }
  }
  cluster {
    seed.zookeeper {
      url = "192.168.30.210:2181,192.168.30.214:2181,192.168.30.202:2181"
      path = "/akka/cluster/seed"
    }

    auto-down-unreachable-after = 10s
  }
}

akka.actor.deployment {
  "/enterprises/*/collector/handler" {
    router = balancing-pool
    nr-of-instances = 10
  }

  "/enterprises/*/tracks/trackReportPVProcessor" {
    router = round-robin-pool
    nr-of-instances = 10
  }

  "/enterprises/*/navigations/navReportPVProsessor" {
    router = round-robin-pool
    nr-of-instances = 10
  }

  "/enterprises/*/tracks/nt/httpHandler" {
    router = round-robin-pool
    nr-of-instances = 200
  }

}

# Disable legacy metrics in akka-cluster.
akka.cluster.metrics.enabled=off

//akka.extensions=["akka.cluster.pubsub.DistributedPubSub"]
akka.extensions=["akka.cluster.metrics.ClusterMetricsExtension"]

#akka.cluster.metrics.native-library-extract-folder=${user.dir}/target/native

akka.cluster.jmx.multi-mbeans-in-same-jvm = on

akka.cluster.pub-sub {
  # Actor name of the mediator actor, /system/distributedPubSubMediator
  name = distributedPubSubMediator

  # Start the mediator on members tagged with this role.
  # All members are used if undefined or empty.
  role = ""

  # The routing logic to use for 'Send'
  # Possible values: random, round-robin, broadcast
  routing-logic = random

  # How often the DistributedPubSubMediator should send out gossip information
  gossip-interval = 1s

  # Removed entries are pruned after this duration
  removed-time-to-live = 5s

  # Maximum number of elements to transfer in one message when synchronizing the registries.
  # Next chunk will be transferred in next round of gossip.
  max-delta-elements = 3000

  # The id of the dispatcher to use for DistributedPubSubMediator actors.
  # If not specified default dispatcher is used.
  # If specified you need to define the settings of the actual dispatcher.
  use-dispatcher = ""
}

# 集群分片设置
akka.cluster.sharding {

  # 两种：ddate(分布式数据，写入本地磁盘)和persistence(存入cassandra)
  state-store-mode = "persistence"

  # 设置on为实体自动重启
  remember-entities = on

  least-shard-allocation-strategy {
    #平衡临界值
    rebalance-threshold = 1
    #最大同时平衡值：一次性停止3个，平均分配到N个节点上
    max-simultaneous-rebalance = 3
  }

  # 集群分片的命名空间名称
  # e.g. '/system/sharding'
  guardian-name = sharding
  # 设置集群中的角色，不设置为所有在所有集群中
  role = ""

  coordinator-failure-backoff = 5 s
  retry-interval = 2 s
  buffer-size = 100000
  handoff-timeout = 60 s
  shard-start-timeout = 10 s
  shard-failure-backoff = 10 s
  entity-restart-backoff = 10 s
  rebalance-interval = 10 s
  journal-plugin-id = ""
  snapshot-plugin-id = ""

  snapshot-after = 1000

  waiting-for-state-timeout = 5 s
  updating-state-timeout = 5 s
  coordinator-singleton = ${akka.cluster.singleton}
  use-dispatcher = ""
}

akka.http.server.remote-address-header = on

systemName = ${systemName}
address ="akka.tcp://NSkyEye@"${clusterAddr}":"${clusterPort}
appUrl=${appUrl}
appPort=${appPort}

neo4j_url="bolt://192.168.30.230"
neo4j_port=7687
neo4j_userName=neo4j
neo4j_password=xuyang
neo4j_maxSession=200

KAFKA_ServiceSize=${KAFKA_ServiceSize}
KAFKA_BROKERS=${KAFKA_BROKERS}":"${KAFKA_BROKERS_PORT}
KAFKA_TOPIC=${KAFKA_TOPIC}

kpi_url="http://"${kpi_url}





include "akka_system.properties"
include "persistence"
//include "persistence-local"